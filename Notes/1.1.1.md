<script type="text/javascript" charset="utf-8" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,
https://vincenttam.github.io/javascripts/MathJaxLocal.js"></script>

# Notes: Matrix Factorization Techniques for Recommender Systems

## 1. Recommender System Strategies

### 1.1 Content Filtering

Creates a profile for each user or product. 

For example, a movie profile could include attributes regarding its **genre**, **actors**, **popularity**. A user profile might include **demographic** information. However, it requires gathering external information that might not be available or easy to collect.

### 1.2 Collaborative Filtering

Analyzes relationships between users and interdependencies among products to identify new user-item associations.

Doman free, but can address data aspects that are often elusive and difficult to profile using content filtering. Generally more accurate than content filtering, but suffers from **cold start** problem, and content filtering is superior in this situation.

The **2** primary areas of collaborative filtering are the **neighborhood methods** and **latent factor models**.

#### 1.2.1 Neighborhood Methods

Centered on computing the relationships between items or users.

1. **Item-oriented** approach evaluates a user's preference for an item based on ratings of "neighboring" items by the same user. A product's neighbors are other products that tend to get similar ratings when rated by the same user.
2. **User-oriented** approach identifies like-minded users who can complement each others' ratings. Finds a user's neighbors who are users that also like the movies the user likes.

#### 1.2.2 Latent Factor Models

Tries to explain the ratings by charactering both items and users on 20-100 factors inferred from the ratings patterns.

For movies, the discovered factors might measure obvious dimensions like genre, quirkiness or completely uninterpretable dimensions. For users, each factor measures how much the user likes movies that score high on the corresponding movie factor.

## 2. Matrix Factorization Methods

Some of the most successful realizations of latent factor models are based on **matrix factorization**. Each item or user has a vector of factors. High correspondence between item and user factors leads to a recommendation.

The input data of recommender system is often a matrix where one dimension represents user and the other represents items of interest.

The most convenient data is high-quality **explicit feedback**: star ratings of stars, thumbs-up and thumbs-down. We call explicit feedback ratings. Usually it's a sparse matrix because a user is likely to only have rated a small percentage of all possible items.

When explicit feedback is not available, we can infer user preferences using **implicit feedback** based on users' past behaviors: purchase history, browsing history, search history. It usually denotes the presence or absence of an action, so a it's a densely filled matrix.

## 3. A Basic Matrix Factorization Model

Maps both users and items to a joint latent factor space of dimensionality $f$, then the user-item interactions are modeled as inner products.

Each item $i$ is associated with a vector $q_i \in \mathbb{R}^f$, and each user $u$ is associated with $p_u \in \mathbb{R}^f$. For $i$, the elements of $q_i$ measure the extent to which the item possesses those factors. For $u$, the elements of $p_u$ measure the extent of interest the user has in items that are high on the corresponding factors. Then user $u$'s rating of item $i$ is approximated by

$$
\hat{r_{ui}}=q_i^\top p_u
$$
This model is closely rated to **singular value decomposition**. But due to high portion of missing values in a sparse ratings matrix, conventional SVD is undefined. Moreover, only considering the known entries can easily lead to overfitting. Imputation is also not desirable.

Recently we mainly model directly the observed data only with regularization. The objective is minimizing the the regularized squared error on the set of known ratings:

$$
\underset{q^*,p^*}{\text{min}}\sum_{(u,i)\in \kappa}(r_{ui}-q_i^\top p_u)^2+\lambda(\lVert q_i\rVert^2+\lVert p_u \rVert^2)
$$
$\kappa$ is the set of  the $(u,i)$ pairs for which $r_{ui}$ is known (the training set).